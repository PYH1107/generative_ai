{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMQcrTiAXYHebRbLloI0cL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PYH1107/generative_ai/blob/main/%EF%BC%A8%EF%BC%B75.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1:\n",
        "importing and installing necessary packages\n",
        "\n",
        "## 1-1\n",
        "1. numpy, pandas, matpotlin, seaborn\n",
        "2. openai, gradio, ollama"
      ],
      "metadata": {
        "id": "dwF8_uVTSKiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "Bum3vDg1yLb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 安裝必要的套件\n",
        "!pip install openai gradio ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvkZFA6JWN9W",
        "outputId": "a36fd942-46f5-4c9f-b786-5bd68ff806c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xai-grok-sdk: 由社群開發的輕量級 Python SDK，專為與 xAI 的 Grok API 互動而設計\n",
        "!pip install requests xai-grok-sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMhxtPhnWY_V",
        "outputId": "38f0125c-1ade-498c-f6a1-b6be7955dd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: xai-grok-sdk in /usr/local/lib/python3.11/dist-packages (0.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2\n",
        "installing ollama & get secret keys"
      ],
      "metadata": {
        "id": "qvP0LlxcW9MP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kOpR0QyWa6L",
        "outputId": "00c6aef4-ea4d-409c-d520-12994013ae18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYpQfLwUYFyg",
        "outputId": "5069ecfd-96d4-4754-f729-57d3c00faa31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOAyUfX9XD_Z",
        "outputId": "e03fc5fd-ef08-4832-e6d4-de5b6831521b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 7cd4618c1faf: 100% ▕▏ 815 MB                         \u001b[K\n",
            "pulling e0a42594d802: 100% ▕▏  358 B                         \u001b[K\n",
            "pulling dd084c7d92a3: 100% ▕▏ 8.4 KB                         \u001b[K\n",
            "pulling 3116c5225075: 100% ▕▏   77 B                         \u001b[K\n",
            "pulling 120007c81bf8: 100% ▕▏  492 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# 嘗試從 Colab Secrets 獲取 API 金鑰\n",
        "try:\n",
        "    grok_api_key = userdata.get('GROK_API_KEY')\n",
        "    print(\"成功獲取 Grok API 金鑰\")\n",
        "except:\n",
        "    grok_api_key = None\n",
        "    print(\"警告: 未找到 Grok API 金鑰\")"
      ],
      "metadata": {
        "id": "CENe2J7XXF6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d147db9d-cdbf-4a29-f20c-8bc5029d6d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "警告: 未找到 Grok API 金鑰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2:\n",
        "setting different documentations\n"
      ],
      "metadata": {
        "id": "esiT5I6MXTf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 創建 config.py 檔案\n",
        "%%writefile config.py\n",
        "\n",
        "# 角色設定\n",
        "CHARACTER_PROMPT = \"\"\"你是一位專業的分析師，具有以下特性：\n",
        "1. 你總是將內容分為不同層次，從最高層次往下分析\n",
        "2. 你會思考每一個層次的邏輯關係\n",
        "3. 你透過層層提問，將每一個層次串連起來，盡量用等分思維來解讀\n",
        "4. 最後，你會透過文字由淺入深地總結整體內容\n",
        "5. 對於數學內容，你會：\n",
        "   - 清晰解釋並列出公式、概念和方程式\n",
        "   - 使用 Markdown 中的 tex 語法撰寫數學公式\n",
        "\n",
        "當用戶詢問任何問題時，你都會遵循以上分析架構來回應。\n",
        "\"\"\"\n",
        "\n",
        "# 模型設定\n",
        "OLLAMA_MODEL = \"llama3\"  # 依您的 Ollama 可用模型調整"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkKeDH-lXS8H",
        "outputId": "909807fe-ff74-4d79-8e4e-b95d0bd06022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_connectors.py\n",
        "%%writefile model_connectors.py\n",
        "import requests\n",
        "from xai_grok_sdk import XAI\n",
        "from config import CHARACTER_PROMPT\n",
        "\n",
        "class ModelConnector:\n",
        "    def __init__(self, api_key=None):\n",
        "        self.api_key = api_key\n",
        "        self.character_prompt = CHARACTER_PROMPT\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        \"\"\"各子類別需實現此方法\"\"\"\n",
        "        raise NotImplementedError(\"請在子類別中實作 generate_response 方法。\")\n",
        "\n",
        "class OllamaConnector(ModelConnector):\n",
        "    def __init__(self, model=\"llama3\"):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.base_url = \"http://localhost:11434/api/generate\"  # Ollama 預設 URL\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        try:\n",
        "            full_prompt = f\"System: {self.character_prompt}\\n\\nUser: {user_input}\\n\\nAssistant:\"\n",
        "            data = {\n",
        "                \"model\": self.model,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"stream\": False\n",
        "            }\n",
        "            response = requests.post(self.base_url, json=data)\n",
        "            response.raise_for_status()\n",
        "            response_json = response.json()\n",
        "            return response_json.get(\"response\", \"無回應\")\n",
        "        except Exception as e:\n",
        "            return f\"Ollama API 錯誤: {str(e)}\"\n",
        "\n",
        "class GrokConnector(ModelConnector):\n",
        "    def __init__(self, api_key, model=\"grok-2-1212\"):\n",
        "        super().__init__(api_key)\n",
        "        self.client = XAI(api_key=api_key, model=model)\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        try:\n",
        "            response = self.client.invoke(\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": self.character_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_input}\n",
        "                ]\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f\"Grok API 錯誤: {str(e)}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBht-fLGXbtm",
        "outputId": "61da3866-4844-4fcf-e29e-76dd33f531b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_connectors.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 創建 chat_history.py 檔案\n",
        "%%writefile chat_history.py\n",
        "\n",
        "class ChatHistory:\n",
        "    def __init__(self):\n",
        "        self.history = []\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        self.history.append((role, content))\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history\n",
        "\n",
        "    def clear_history(self):\n",
        "        self.history = []\n",
        "\n",
        "    def format_for_display(self):\n",
        "        formatted = []\n",
        "        for role, content in self.history:\n",
        "            formatted.append((role, content))\n",
        "        return formatted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC6VaE1BXfaz",
        "outputId": "1e5c067c-1140-41d4-f755-a3882a730353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting chat_history.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "\n",
        "import gradio as gr\n",
        "from model_connectors import OllamaConnector, GrokConnector\n",
        "from chat_history import ChatHistory\n",
        "\n",
        "class ChatbotApp:\n",
        "    def __init__(self, grok_api_key=None):\n",
        "        self.grok_api_key = grok_api_key\n",
        "        self.chat_history = ChatHistory()\n",
        "\n",
        "        # 初始化連接器\n",
        "        self.connectors = {\n",
        "            \"Ollama\": OllamaConnector(),\n",
        "            \"Grok\": GrokConnector(api_key=grok_api_key) if grok_api_key else None\n",
        "        }\n",
        "\n",
        "        self.current_model = \"Ollama\"\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        if model_name in self.connectors and self.connectors[model_name] is not None:\n",
        "            self.current_model = model_name\n",
        "            return f\"已切換至 {model_name} 模型\"\n",
        "        else:\n",
        "            return f\"錯誤: {model_name} 模型尚未啟用或金鑰未提供\"\n",
        "\n",
        "    def chat(self, user_input, history):\n",
        "        if not user_input.strip():\n",
        "            return history\n",
        "\n",
        "        connector = self.connectors[self.current_model]\n",
        "        if connector is None:\n",
        "            response = f\"錯誤: {self.current_model} 連接器未正確初始化\"\n",
        "        else:\n",
        "            response = connector.generate_response(user_input)\n",
        "\n",
        "        self.chat_history.add_message(\"User\", user_input)\n",
        "        self.chat_history.add_message(\"AI\", response)\n",
        "\n",
        "        return self.chat_history.format_for_display()\n",
        "\n",
        "    def clear_chat(self):\n",
        "        self.chat_history.clear_history()\n",
        "        return []\n",
        "\n",
        "    def launch_app(self):\n",
        "        with gr.Blocks(title=\"分層思考 AI 聊天機器人\") as app:\n",
        "            gr.Markdown(\"# 分層思考分析型 AI 聊天機器人\")\n",
        "\n",
        "            with gr.Row():\n",
        "                model_radio = gr.Radio(\n",
        "                    [\"Ollama\", \"Grok\"],\n",
        "                    label=\"選擇 AI 模型\",\n",
        "                    value=\"Ollama\"\n",
        "                )\n",
        "                model_status = gr.Textbox(label=\"模型狀態\", value=\"使用 Ollama 模型中\")\n",
        "\n",
        "            chatbot = gr.Chatbot(label=\"對話\", height=500)\n",
        "\n",
        "            with gr.Row():\n",
        "                user_input = gr.Textbox(label=\"輸入您的問題\", placeholder=\"請輸入您想分析的問題...\", lines=2)\n",
        "                clear_btn = gr.Button(\"清除對話\")\n",
        "\n",
        "            model_radio.change(\n",
        "                fn=self.set_model,\n",
        "                inputs=model_radio,\n",
        "                outputs=model_status\n",
        "            )\n",
        "\n",
        "            user_input.submit(\n",
        "                fn=self.chat,\n",
        "                inputs=[user_input, chatbot],\n",
        "                outputs=chatbot\n",
        "            ).then(\n",
        "                fn=lambda: \"\",\n",
        "                outputs=user_input\n",
        "            )\n",
        "\n",
        "            clear_btn.click(\n",
        "                fn=self.clear_chat,\n",
        "                outputs=chatbot\n",
        "            )\n",
        "\n",
        "        return app\n",
        "\n",
        "# 啟動點\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"啟動 AI 聊天機器人\")\n",
        "    parser.add_argument(\"--grok_key\", type=str, help=\"Grok API 金鑰\", default=None)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    app = ChatbotApp(\n",
        "        grok_api_key=args.grok_key\n",
        "    )\n",
        "    app.launch_app().launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYMCIGjvXglW",
        "outputId": "f447026b-daa5-46d7-8833-643c7b982e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 創建 app.py 檔案 - 主要執行檔\n",
        "%%writefile app.py\n",
        "\n",
        "from google.colab import userdata\n",
        "from main import ChatbotApp\n",
        "\n",
        "try:\n",
        "    grok_api_key = userdata.get('groq')\n",
        "except:\n",
        "    grok_api_key = None\n",
        "    print(\"警告: 未找到 Grok API 金鑰\")\n",
        "\n",
        "# 創建並啟動應用\n",
        "app = ChatbotApp(\n",
        "    grok_api_key=grok_api_key\n",
        ")\n",
        "\n",
        "# 啟動 Gradio 應用\n",
        "app.launch_app().launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUiGxNBPXjLc",
        "outputId": "3e67e0ae-41b5-4b4a-f642-ed8d425683a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 檢查所有文件是否已正確生成\n",
        "!ls -la\n",
        "\n",
        "# 執行應用\n",
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls76pFa2Xk1a",
        "outputId": "9b5c3f2b-4566-4117-c9f9-13b97eab6513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 52\n",
            "drwxr-xr-x 1 root root 4096 May  6 15:46 .\n",
            "drwxr-xr-x 1 root root 4096 May  6 15:25 ..\n",
            "-rw-r--r-- 1 root root  323 May  6 16:14 app.py\n",
            "-rw-r--r-- 1 root root  442 May  6 16:14 chat_history.py\n",
            "drwxr-xr-x 4 root root 4096 May  2 13:33 .config\n",
            "-rw-r--r-- 1 root root  672 May  6 16:14 config.py\n",
            "drwxr-xr-x 2 root root 4096 May  6 15:46 .gradio\n",
            "-rw-r--r-- 1 root root 3037 May  6 16:14 main.py\n",
            "-rw-r--r-- 1 root root 1820 May  6 16:14 model_connectors.py\n",
            "-rw------- 1 root root 4463 May  6 16:14 nohup.out\n",
            "drwxr-xr-x 2 root root 4096 May  6 15:45 __pycache__\n",
            "drwxr-xr-x 1 root root 4096 May  2 13:33 sample_data\n",
            "警告: 未找到 Grok API 金鑰\n",
            "/content/main.py:57: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"對話\", height=500)\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://aa206f01f9b156554b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3019, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 17, in <module>\n",
            "    app.launch_app().launch(share=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2925, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3023, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\", line 64, in close\n",
            "    def close(self):\n",
            "\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://aa206f01f9b156554b.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}